{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Comprehend Custom\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction\n",
    "2. Setup\n",
    "3. Download data\n",
    "4. Train a recognizer\n",
    "5. Inference\n",
    "6. Model evaluation\n",
    "7. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have custom entities that we would like to detect in our documents.  Instead of being limited to the entity types that are detected by Comprehend out of the box, we can build a custom Comprehend model to detect any entity type of interest. \n",
    "\n",
    "You can choose one of two ways to provide data to Amazon Comprehend in order to train a custom entity recognition model:\n",
    "\n",
    "   - Annotations — Provides the location of your entities in a large number of documents so Amazon Comprehend can train on both the entity and its context. To create a model which can be used to analyze PDF, Word and plain text documents, you must train your recognizer using PDF annotations.\n",
    "\n",
    "   - Entity Lists (Plain Text Only) — Lists the specific entities so Amazon Comprehend can train to identify your custom entities. Note: Entity lists can only be used for plain text documents.\n",
    "In both cases, Amazon Comprehend will learn about the kind of documents and the context where the entities occur and build a recognizer that can generalize to new entities in documents at inference.  To learn more refer to the official AWS documentation [here](https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will show how to use annotations to build a custom Comprehend model to detect five custom entities in insurance documents (e.g. DateOfLoss), and how to use that custom model to detect entities in an unlabled document.  We will demonstrate training the model with PDF annotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.19.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.7 MB)\n",
      "     |████████████████████████████████| 8.7 MB 21.7 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.19.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# we'll use a custom utils module for visualizing annotations on pdfs\n",
    "!pip install --upgrade pymupdf\n",
    "module_path = os.path.join(os.path.abspath(os.path.join('.')), 'helperPackage')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from pdfhelper.PDFHelper import PDFHelper\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def get_ssm_parameter(parameter_name):\n",
    "    return boto3.client('ssm').get_parameter(Name=parameter_name)['Parameter']['Value']\n",
    "\n",
    "def split_s3_uri(uri):\n",
    "    \"\"\"return (bucket, key) tuple from s3 uri like 's3://bucket/prefix/file.txt' \"\"\"\n",
    "    return uri.replace('s3://','').split('/',1)\n",
    "\n",
    "def s3_object_from_uri(uri):\n",
    "    \"\"\"Initialize a boto3 s3 Object instance from a URI\"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    return s3.Object(*split_s3_uri(uri))\n",
    "\n",
    "def s3_contents_from_uri(uri, decode=True):\n",
    "    \"\"\"Read contents from S3 object into memory\"\"\"\n",
    "    data = s3_object_from_uri(uri).get()['Body'].read()\n",
    "    return data.decode() if decode else data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create annoations for PDF documents, you can use [Amazon SageMaker GroundTruth](https://aws.amazon.com/sagemaker/groundtruth/) - a fully managed data labeling service that makes it easy to build highly accurate training datasets for machine learning.\n",
    "\n",
    "For this tutorial, we have already annotated the PDFs, in their native form (i.e. without converting to plain text) using SageMaker GroundTruth. (To set up your own annotation job, refer to the resources in the **Summary/Resources** section of this notebook)\n",
    "\n",
    "The Ground Truth job generates three paths we will need for training our Comprehend custom model.\n",
    "1. Sources: Path to the input PDFs\n",
    "2. Annotations: Path to the annotation jsons containing the labeled entity information\n",
    "3. Manifest: Points to the location of the annotations and source PDFs.  You will use this manifest file to create an Amazon Comprehend custom entity recognition training job and train your custom model.  Manifests are saved in s3://comprehend-semi-structured-documents-us-east-1--<AWS Account number>/output/your labeling job/manifests/output/\n",
    "    \n",
    "Let's get some example outputs from that annotation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSETS_S3_PREFIX = get_ssm_parameter('AssetsS3Prefix')\n",
    "\n",
    "# output S3 bucket to store results in\n",
    "OUTPUT_BUCKET_NAME = get_ssm_parameter('OutputBucketName')\n",
    "\n",
    "# trained recognizer\n",
    "TRAINED_RECOGNIZER_ARN = get_ssm_parameter('ModelArn')\n",
    "\n",
    "# Information about the training data and how the SageMaker Ground Truth job output looks in S3\n",
    "TRAINING_DOCS_S3_URI_PREFIX = os.path.join(ASSETS_S3_PREFIX, 'documents/')\n",
    "ANNOTATIONS_S3_URI_PREFIX = os.path.join(ASSETS_S3_PREFIX, 'annotations/annotations/consolidated-annotation/consolidation-response/iteration-1/annotations/')\n",
    "MANIFEST_S3_URI = os.path.join(ASSETS_S3_PREFIX, 'annotations/manifests/output/output.manifest')\n",
    "LABEL_ATTRIBUTE_NAME = 'claim-full-job-labeling-job-20211019T163532'\n",
    "\n",
    "\n",
    "# local directory containing example training data artifacts (pdfs, annotations, manifest) referenced in this notebook\n",
    "LOCAL_ARTIFACTS_DIR = 'ComprehendCustom-Artifacts'\n",
    "# local path to store results in\n",
    "LOCAL_OUTPUT_DIR = 'tmp/ComprehendCustom'\n",
    "# set up tmp dir under the working directory\n",
    "!mkdir -p {LOCAL_OUTPUT_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlockReferences': [{'BlockId': 'fe83c69f-b32f-423f-ac5e-f37959a8cf25',\n",
       "   'ChildBlocks': [{'BeginOffset': 0,\n",
       "     'EndOffset': 10,\n",
       "     'ChildBlockId': '331afc54-fbc0-466e-9b16-b614cb81d3bb'}],\n",
       "   'BeginOffset': 0,\n",
       "   'EndOffset': 10}],\n",
       " 'Text': '03-28-2007',\n",
       " 'Type': 'DateOfForm',\n",
       " 'Score': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's preview a portion of the manifest file\n",
    "\n",
    "# We will find the line of the manifest corresponding to a particular input document\n",
    "document_s3_uri = os.path.join(ASSETS_S3_PREFIX, 'documents','INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00000.pdf')\n",
    "\n",
    "manifest_data = [json.loads(obj) for obj in s3_contents_from_uri(MANIFEST_S3_URI).splitlines()]\n",
    "\n",
    "manifest_line = [r for r in manifest_data if r['source-ref']==document_s3_uri][0]\n",
    "# manifest_line\n",
    "\n",
    "# Let's download the annotation file and look at a sample annotation\n",
    "\n",
    "annotations_uri = manifest_line[LABEL_ATTRIBUTE_NAME]['annotation-ref']\n",
    "\n",
    "annotations = json.loads(s3_contents_from_uri(annotations_uri))\n",
    "annotations['Entities'][0]\n",
    "\n",
    "## Uncomment the following line to see more of the annotated entities:\n",
    "# annotations['Entities']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the custom GroundTruth job generates a PDF annotation that captures block-level information about the entity.  Such block-level information provides the precise positional coordinates of the entity (with the child blocks representing each word within the entity block).  This is distinct from a standard GroundTruth job in which the data in the PDF is flattened to textual format and only offset information - but not precise coordinate information - is captured during annotation.  The rich positional information we obtain with this custom annotation paradigm will allow us to train a more accurate model. \n",
    "\n",
    "The manifest that's generated from this type of job is called an Augmented Manifest, as opposed to a CSV that's used for standard annotations.\n",
    "For more information, see: https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyMuPDF 1.19.2: Python bindings for the MuPDF 1.19.0 library.\n",
      "Version date: 2021-11-20 00:00:01.\n",
      "Built for Python 3.7 on linux (64-bit).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"800\"\n",
       "            src=\"tmp/ComprehendCustom/annotated.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4c53044510>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the annotated pdf inline\n",
    "\n",
    "original_file = f'{LOCAL_ARTIFACTS_DIR}/ex_pdfs/INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00000.pdf'\n",
    "annotated_file = f'{LOCAL_OUTPUT_DIR}/annotated.pdf'\n",
    "\n",
    "# using a custom module (PDFHelper) to add annotations the file before displaying\n",
    "PDFHelper.add_annotations_to_file(annotations, original_file, annotated_file)\n",
    "IFrame(annotated_file, width=600, height=800)\n",
    "\n",
    "# Note: you may need to zoom in to read the label names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyMuPDF 1.19.2: Python bindings for the MuPDF 1.19.0 library.\n",
      "Version date: 2021-11-20 00:00:01.\n",
      "Built for Python 3.7 on linux (64-bit).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"800\"\n",
       "            src=\"tmp/ComprehendCustom/INSR_pm_hipaa_1_pii_00048_annotated.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4c518cae50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets look at another annotated sample\n",
    "\n",
    "# changed the document s3 uri\n",
    "document_s3_uri = os.path.join(ASSETS_S3_PREFIX, 'documents','INSR_pm_hipaa_1_pii_00048.pdf')\n",
    "\n",
    "# get the annotations data\n",
    "manifest_data = [json.loads(obj) for obj in s3_contents_from_uri(os.path.join(ASSETS_S3_PREFIX, 'annotations/manifests/output/output.manifest')).splitlines()]\n",
    "manifest_line = [r for r in manifest_data if r['source-ref']==document_s3_uri][0]\n",
    "annotations_uri = manifest_line[LABEL_ATTRIBUTE_NAME]['annotation-ref']\n",
    "annotations = json.loads(s3_contents_from_uri(annotations_uri))\n",
    "\n",
    "original_file = f'{LOCAL_ARTIFACTS_DIR}/ex_pdfs/INSR_pm_hipaa_1_pii_00048.pdf'\n",
    "annotated_file =f'{LOCAL_OUTPUT_DIR}/INSR_pm_hipaa_1_pii_00048_annotated.pdf'\n",
    "\n",
    "# using a custom module (PDFHelper) to add annotations the file before displaying\n",
    "PDFHelper.add_annotations_to_file(annotations, original_file, annotated_file)\n",
    "IFrame(annotated_file, width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train a recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In order to train a custom recognizer, SageMaker will need access to a role that has policy permissions to the s3 location where your data is. We have already set up this role - see information on the process in the Appendix section of the workshop guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An augmented manifest file must be formatted in JSON Lines format. In JSON Lines format, each line in the file is a complete JSON object followed by a newline separator.\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source-ref': 's3://ee-assets-prod-us-east-1/modules/b2d6c897c659445583c2edb826183e8e/v1/documents/INSR_pm_hipaa_1_pii_00048.pdf',\n",
       " 'page': '1',\n",
       " 'metadata': {'pages': '1',\n",
       "  'use-textract-only': False,\n",
       "  'labels': ['DateOfForm',\n",
       "   'DateOfLoss',\n",
       "   'NameOfInsured',\n",
       "   'LocationOfLoss',\n",
       "   'InsuredMailingAddress']},\n",
       " 'annotator-metadata': {'Info': 'Sample information',\n",
       "  'Due Date': 'Sample date value 12/12/1212'},\n",
       " 'claim-full-job-labeling-job-20211019T163532': {'annotation-ref': 's3://ee-assets-prod-us-east-1/modules/b2d6c897c659445583c2edb826183e8e/v1/annotations/annotations/consolidated-annotation/consolidation-response/iteration-1/annotations/INSR_pm_hipaa_1_pii_00048-1-51948fd8-ann.json'},\n",
       " 'claim-full-job-labeling-job-20211019T163532-metadata': {'type': 'groundtruth/custom',\n",
       "  'job-name': 'claim-full-job-labeling-job-20211019t163532',\n",
       "  'human-annotated': 'yes',\n",
       "  'creation-date': '2021-10-20T06:28:22.043000'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at an entry within this augmented manifest file.\n",
    "manifest_line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "\n",
    "- There are 5 labeling types associated with this job: DateOfForm, DateOfLoss, NameOfInsured, LocationOfLoss, and InsuredMailingAddress\n",
    "\n",
    "- The manifest file makes reference to both the source PDF location and the annotation location\n",
    "\n",
    "- Metadata about the annotation job (e.g. creation date) is captured.\n",
    "\n",
    "- Use-textract-only is set to False, meaning the annotation tool will decide whether to use PDFPlumber (for a native PDF) or Amazon Textract (for a scanned PDF). If it were set to true, Textract would be used in either case (more costly but potentially more accurate).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:comprehend:us-east-1:469755836051:entity-recognizer/recognizer-example-51396fc8-53ee-43e9-9d8d-336e61772d92'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train a recognizer\n",
    "\n",
    "comprehend = boto3.client('comprehend')\n",
    "response = comprehend.create_entity_recognizer(\n",
    "    RecognizerName=\"recognizer-example-{}\".format(str(uuid.uuid4())),\n",
    "    LanguageCode=\"en\",\n",
    "    DataAccessRoleArn=get_ssm_parameter('ComprehendRoleArn'),\n",
    "    InputDataConfig={\n",
    "        \"DataFormat\": \"AUGMENTED_MANIFEST\",\n",
    "        \"EntityTypes\": [\n",
    "            {\n",
    "                \"Type\": \"DateOfForm\"\n",
    "            },\n",
    "            {\n",
    "                \"Type\": \"DateOfLoss\"\n",
    "            },\n",
    "            {\n",
    "                \"Type\": \"NameOfInsured\"\n",
    "            },\n",
    "            {\n",
    "                \"Type\": \"LocationOfLoss\"\n",
    "            },\n",
    "            {\n",
    "                \"Type\": \"InsuredMailingAddress\"\n",
    "            }\n",
    "        ],\n",
    "        \"AugmentedManifests\": [\n",
    "            {\n",
    "                'S3Uri': MANIFEST_S3_URI,\n",
    "                'AnnotationDataS3Uri': ANNOTATIONS_S3_URI_PREFIX,\n",
    "                'SourceDocumentsS3Uri': TRAINING_DOCS_S3_URI_PREFIX,\n",
    "                'AttributeNames': [LABEL_ATTRIBUTE_NAME],\n",
    "                'DocumentType': 'SEMI_STRUCTURED_DOCUMENT',\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "recognizer_arn = response[\"EntityRecognizerArn\"]\n",
    "recognizer_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are creating a recognizer to recognize all five types of entities.  Of course, we could have used a subset of these entities if we preferred.  You can use up to 25 entities. \n",
    "\n",
    "The details of each parameter are given below (source: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.create_entity_recognizer)\n",
    "\n",
    "**DataFormat (string) --**\n",
    "\n",
    "    The format of your training data:\n",
    "\n",
    "    COMPREHEND_CSV : A CSV file that supplements your training documents. The CSV file contains information about the custom entities that your trained model will detect. The required format of the file depends on whether you are providing annotations or an entity list. If you use this value, you must provide your CSV file by using either the \n",
    "    Annotations or EntityList parameters. You must provide your training documents by using the Documents parameter.\n",
    "\n",
    "    AUGMENTED_MANIFEST : A labeled dataset that is produced by Amazon SageMaker Ground Truth. This file is in JSON lines format. Each line is a complete JSON object that contains a training document and its labels. Each label annotates a named entity in the training document. If you use this value, you must provide the AugmentedManifests parameter in your request.\n",
    "    If you don't specify a value, Amazon Comprehend uses COMPREHEND_CSV as the default.\n",
    "\n",
    "**EntityTypes (list) -- [REQUIRED]**\n",
    "\n",
    "    The entity types in the labeled training data that Amazon Comprehend uses to train the custom entity recognizer. Any entity types that you don't specify are ignored.\n",
    "\n",
    "    A maximum of 25 entity types can be used at one time to train an entity recognizer.\n",
    "\n",
    "**S3Uri (string) -- [REQUIRED]**\n",
    "\n",
    "    The Amazon S3 location of the augmented manifest file.\n",
    "\n",
    "**AnnotationDataS3Uri (string) --**\n",
    "\n",
    "    The S3 prefix to the annotation files that are referred in the augmented manifest file.\n",
    "\n",
    "**SourceDocumentsS3Uri (string) --**\n",
    "\n",
    "    The S3 prefix to the source files (PDFs) that are referred to in the augmented manifest file.\n",
    "\n",
    "**AttributeNames (list) -- [REQUIRED]**\n",
    "\n",
    "    The JSON attribute that contains the annotations for your training documents. The number of attribute names that you specify depends on whether your augmented manifest file is the output of a single labeling job or a chained labeling job.\n",
    "\n",
    "    If your file is the output of a single labeling job, specify the LabelAttributeName key that was used when the job was created in Ground Truth.\n",
    "\n",
    "    If your file is the output of a chained labeling job, specify the LabelAttributeName key for one or more jobs in the chain. Each LabelAttributeName key provides the annotations from an individual job.\n",
    "\n",
    "**DocumentType (string) --**\n",
    "\n",
    "    The type of augmented manifest. PlainTextDocument or SemiStructuredDocument. If you don't specify, the default is PlainTextDocument.\n",
    "\n",
    "    PLAIN_TEXT_DOCUMENT A document type that represents any unicode text that is encoded in UTF-8.\n",
    "\n",
    "    SEMI_STRUCTURED_DOCUMENT A document type with positional and structural context, like a PDF. For training with Amazon Comprehend, only PDFs are supported. For inference, Amazon Comprehend support PDFs, DOCX and TXT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EntityRecognizerArn': 'arn:aws:comprehend:us-east-1:469755836051:entity-recognizer/recognizer-example-51396fc8-53ee-43e9-9d8d-336e61772d92',\n",
       " 'LanguageCode': 'en',\n",
       " 'Status': 'SUBMITTED',\n",
       " 'SubmitTime': datetime.datetime(2021, 12, 2, 21, 7, 51, 294000, tzinfo=tzlocal()),\n",
       " 'InputDataConfig': {'DataFormat': 'AUGMENTED_MANIFEST',\n",
       "  'EntityTypes': [{'Type': 'DateOfForm'},\n",
       "   {'Type': 'DateOfLoss'},\n",
       "   {'Type': 'NameOfInsured'},\n",
       "   {'Type': 'LocationOfLoss'},\n",
       "   {'Type': 'InsuredMailingAddress'}],\n",
       "  'AugmentedManifests': [{'S3Uri': 's3://ee-assets-prod-us-east-1/modules/b2d6c897c659445583c2edb826183e8e/v1/annotations/manifests/output/output.manifest',\n",
       "    'Split': 'TRAIN',\n",
       "    'AttributeNames': ['claim-full-job-labeling-job-20211019T163532'],\n",
       "    'AnnotationDataS3Uri': 's3://ee-assets-prod-us-east-1/modules/b2d6c897c659445583c2edb826183e8e/v1/annotations/annotations/consolidated-annotation/consolidation-response/iteration-1/annotations/',\n",
       "    'SourceDocumentsS3Uri': 's3://ee-assets-prod-us-east-1/modules/b2d6c897c659445583c2edb826183e8e/v1/documents/',\n",
       "    'DocumentType': 'SEMI_STRUCTURED_DOCUMENT'}]},\n",
       " 'DataAccessRoleArn': 'arn:aws:iam::469755836051:role/ComprehendRole'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check on the status of the submitted training job\n",
    "\n",
    "# All recognizers\n",
    "comprehend = boto3.client('comprehend')\n",
    "\n",
    "recognizers = comprehend.list_entity_recognizers()\n",
    "# View the last submitted job\n",
    "recognizers['EntityRecognizerPropertiesList'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waiting for training job completion\n",
    "\n",
    "This snippet below could be used to print out the status of your model.\n",
    "\n",
    "We've already trained a model in the account in advance, using the exact same dataset, so we will use that model and continue with the notebook instead of waiting for the new model training job to finish.\n",
    "\n",
    "```\n",
    "# check status of custom model training periodically until complete\n",
    "recognizer_arn = recognizers['EntityRecognizerPropertiesList'][-1]['EntityRecognizerArn']\n",
    "\n",
    "while True:\n",
    "    response = comprehend.describe_entity_recognizer(\n",
    "        EntityRecognizerArn=recognizer_arn\n",
    "    )\n",
    "\n",
    "    status = response[\"EntityRecognizerProperties\"][\"Status\"]\n",
    "    if \"IN_ERROR\" == status:\n",
    "        print('TRAINING ERROR')\n",
    "        break\n",
    "    if \"TRAINED\" == status:\n",
    "        print('TRAINING COMPLETE')\n",
    "        break\n",
    "    print(status)\n",
    "    time.sleep(60)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run inference with our trained model on a document that was not part of the training procedure. This asynchronous API can be used for standard or custom NER. If it is being used for custom NER (as it is here) we must pass the ARN of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobId': '62ff8b5dbc0e0883eb4df53fc9e02226',\n",
       " 'JobArn': 'arn:aws:comprehend:us-east-1:469755836051:entities-detection-job/62ff8b5dbc0e0883eb4df53fc9e02226',\n",
       " 'JobStatus': 'SUBMITTED',\n",
       " 'ResponseMetadata': {'RequestId': '8f1f76bd-f750-49a7-962e-962fc06f0856',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8f1f76bd-f750-49a7-962e-962fc06f0856',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '177',\n",
       "   'date': 'Thu, 02 Dec 2021 21:08:46 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start entities detection job\n",
    "\n",
    "# This asynchronous API can be used for standard or custom NER.\n",
    "# To use custom NER, pass the ARN of the trained model.\n",
    "\n",
    "response = comprehend.start_entities_detection_job(\n",
    "    EntityRecognizerArn=TRAINED_RECOGNIZER_ARN,\n",
    "    JobName=\"Detection-Job-{}\".format(str(uuid.uuid4())),\n",
    "    LanguageCode=\"en\",\n",
    "    DataAccessRoleArn=get_ssm_parameter('ComprehendRoleArn'),\n",
    "    InputDataConfig={\n",
    "        \"InputFormat\": \"ONE_DOC_PER_FILE\",\n",
    "        \"S3Uri\": os.path.join(ASSETS_S3_PREFIX, 'holdout/')\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        \"S3Uri\": f's3://{OUTPUT_BUCKET_NAME}/custom_comprehend/'\n",
    "    }\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**S3Uri (string) -- [REQUIRED]**\n",
    "    The Amazon S3 URI for the input data. The URI must be in same region as the API endpoint that you are calling. The URI can point to a single input file or it can provide the prefix for a collection of data files.\n",
    "\n",
    "    For example, if you use the URI S3://bucketName/prefix , if the prefix is a single file, Amazon Comprehend uses that file as input. If more than one file begins with the prefix, Amazon Comprehend uses all of them as input.\n",
    "\n",
    "**InputFormat (string) --**\n",
    "    Specifies how the text in an input file should be processed:\n",
    "\n",
    "    ONE_DOC_PER_FILE - Each file is considered a separate document. Use this option when you are processing large documents, such as newspaper articles or scientific papers.\n",
    "    ONE_DOC_PER_LINE - Each line in a file is considered a separate document. Use this option when you are processing many short documents, such as text messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Waiting for detection job completion**\n",
    "\n",
    "This code snippet could be used to periodically check the status of your detection job and wait until it finishes.\n",
    "\n",
    "We've already done a detection with this model so we will take a look at those results instead of using this code to wait for the job to finish.\n",
    "\n",
    "```\n",
    "while True:\n",
    "    job = comprehend.describe_entities_detection_job(\n",
    "        JobId=response['JobId']\n",
    "    )\n",
    "    \n",
    "    status = job[\"EntitiesDetectionJobProperties\"][\"JobStatus\"]\n",
    "    if \"IN_ERROR\" == status:\n",
    "        print('DETECTION ERROR')\n",
    "        break\n",
    "    if \"COMPLETED\" == status:\n",
    "        print('DETECTION COMPLETE')\n",
    "        break\n",
    "    print(status)\n",
    "    time.sleep(60)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017_NativePDF.out\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "# Get the output from the detection job\n",
    "\n",
    "# download pre-generated inference output for INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017\n",
    "INFERENCE_RESULTS_S3_URI = os.path.join(ASSETS_S3_PREFIX, 'detection/output/output.tar.gz')\n",
    "\n",
    "# Detection job output is at {LOCAL_ARTIFACTS_DIR}/inference_output/output.tar.gz\n",
    "\n",
    "!mkdir -p {LOCAL_OUTPUT_DIR}/inference_output/\n",
    "!tar -xvzf {LOCAL_ARTIFACTS_DIR}/inference_output/output.tar.gz -C {LOCAL_OUTPUT_DIR}/inference_output/\n",
    "\n",
    "INFERENCE_RESULTS_PATH = os.path.join(LOCAL_OUTPUT_DIR, 'inference_output/INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017_NativePDF.out')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BlockReferences</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 0, 'BlockId': '47c99141-baa5-...</td>\n",
       "      <td>0.995752</td>\n",
       "      <td>02-11-2008</td>\n",
       "      <td>DateOfForm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 0, 'BlockId': '382f2f85-e1c8-...</td>\n",
       "      <td>0.991779</td>\n",
       "      <td>03-05-2005 17:06:44</td>\n",
       "      <td>DateOfLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 0, 'BlockId': 'fb9b875f-7a79-...</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>Jaleesa Gonzalez</td>\n",
       "      <td>NameOfInsured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 23, 'BlockId': 'fb9b875f-7a79...</td>\n",
       "      <td>0.912355</td>\n",
       "      <td>, Vermont</td>\n",
       "      <td>InsuredMailingAddress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 0, 'BlockId': '867de541-8fc2-...</td>\n",
       "      <td>0.986978</td>\n",
       "      <td>312 Fernwood Alley</td>\n",
       "      <td>LocationOfLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'BeginOffset': 18, 'BlockId': 'df9a3ba3-b843...</td>\n",
       "      <td>0.997802</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>LocationOfLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'BeginOffset': 9, 'BlockId': '3bb70a0f-f970-...</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>United States</td>\n",
       "      <td>LocationOfLoss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     BlockReferences     Score  \\\n",
       "0  [{'BeginOffset': 0, 'BlockId': '47c99141-baa5-...  0.995752   \n",
       "1  [{'BeginOffset': 0, 'BlockId': '382f2f85-e1c8-...  0.991779   \n",
       "2  [{'BeginOffset': 0, 'BlockId': 'fb9b875f-7a79-...  0.998773   \n",
       "3  [{'BeginOffset': 23, 'BlockId': 'fb9b875f-7a79...  0.912355   \n",
       "4  [{'BeginOffset': 0, 'BlockId': '867de541-8fc2-...  0.986978   \n",
       "5  [{'BeginOffset': 18, 'BlockId': 'df9a3ba3-b843...  0.997802   \n",
       "6  [{'BeginOffset': 9, 'BlockId': '3bb70a0f-f970-...  0.999795   \n",
       "\n",
       "                  Text                   Type  \n",
       "0           02-11-2008             DateOfForm  \n",
       "1  03-05-2005 17:06:44             DateOfLoss  \n",
       "2     Jaleesa Gonzalez          NameOfInsured  \n",
       "3            , Vermont  InsuredMailingAddress  \n",
       "4   312 Fernwood Alley         LocationOfLoss  \n",
       "5         South Dakota         LocationOfLoss  \n",
       "6        United States         LocationOfLoss  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the inference on a new example document\n",
    "import pandas as pd\n",
    "\n",
    "INFERENCE_RESULTS_PATH = os.path.join(LOCAL_OUTPUT_DIR, 'inference_output/INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017_NativePDF.out')\n",
    "# fname = f'{LOCAL_DIR}/inference_output/INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017_NativePDF.out' \n",
    "with open(INFERENCE_RESULTS_PATH) as f:\n",
    "    detection_output = json.load(f)\n",
    "\n",
    "entities_list = detection_output['Entities']\n",
    "\n",
    "pd.DataFrame(entities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BlockReferences': [{'BeginOffset': 0,\n",
       "   'BlockId': '47c99141-baa5-4df9-8aa3-ed24d4675c8d',\n",
       "   'ChildBlocks': [{'BeginOffset': 0,\n",
       "     'ChildBlockId': '6e7c051e-3d96-4b2f-93bd-023ed5f8991d',\n",
       "     'EndOffset': 10}],\n",
       "   'EndOffset': 10}],\n",
       " 'Score': 0.9957520982071427,\n",
       " 'Text': '02-11-2008',\n",
       " 'Type': 'DateOfForm'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the model prediction output format closely resembles the annotation output format shown above.\n",
    "entities_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyMuPDF 1.19.2: Python bindings for the MuPDF 1.19.0 library.\n",
      "Version date: 2021-11-20 00:00:01.\n",
      "Built for Python 3.7 on linux (64-bit).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"800\"\n",
       "            src=\"ComprehendCustom-Artifacts/detection_annotated.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fca2cee5790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualize the the labels predicted by our model for this new example pdf\n",
    "\n",
    "holdout_pdf = f'{LOCAL_ARTIFACTS_DIR}/ex_pdfs/INSR_ACORD-Property-Loss-Notice-12.05.16_1_pii_00017.pdf'\n",
    "annotated_file =f'{LOCAL_OUTPUT_DIR}/detection_annotated.pdf'\n",
    "PDFHelper.add_annotations_to_file(detection_output, holdout_pdf, annotated_file)\n",
    "IFrame(annotated_file, width=600, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehend provides model performance metrics for a trained model, which indiciates how well the trained model is expected to make predictions using similar inputs.\n",
    "\n",
    "For detailed description of the performance metrics and how they are calculated, see: https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look at the metrics for the model trained in advance of the workshop, using the same dataset\n",
    "\n",
    "trained_recognizer = comprehend.describe_entity_recognizer(\n",
    "    EntityRecognizerArn=TRAINED_RECOGNIZER_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': 1.0, 'Recall': 0.9943181818181818, 'F1Score': 0.9971509971509972}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global evaluation metrics\n",
    "trained_recognizer['EntityRecognizerProperties']['RecognizerMetadata']['EvaluationMetrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateOfForm\n",
      "{'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0}\n",
      "\n",
      "DateOfLoss\n",
      "{'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0}\n",
      "\n",
      "InsuredMailingAddress\n",
      "{'Precision': 1.0, 'Recall': 0.9814814814814815, 'F1Score': 0.9906542056074767}\n",
      "\n",
      "LocationOfLoss\n",
      "{'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0}\n",
      "\n",
      "NameOfInsured\n",
      "{'Precision': 1.0, 'Recall': 1.0, 'F1Score': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Per entity metrics\n",
    "entity_metrics = trained_recognizer['EntityRecognizerProperties']['RecognizerMetadata']['EntityTypes']\n",
    "for entity in entity_metrics:\n",
    "    print(entity['Type'])\n",
    "    print(entity['EvaluationMetrics'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Precision, Recall, and F1 Score, 1.0 is the highest possible score. Most of these metrics for the trained model are at or close to 1.0 which indicates the model is accurately predicting custom entities on a set of test documents randomly selected (and held out from the training data) by Comprehend during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "In addition to the standard set of entities recognized by Amazon Comprehend's standard entity detection capabilities, Comprehend enables you to train and use your own custom models for detecting user-defined entities specific to your business use case directly on PDF documents.\n",
    "\n",
    "In this notebook you used a dataset of PDFs annotated with SageMaker Ground Truth to train a entity detection model in Comprehend. The standard entity detection capabilities of Comprehend did not recognize entities required in thise specific insurance form use case such as ”LocationOfLoss“. After training, the resulting Comprehend model can be used to reliably detect these custom entities in new documents.\n",
    "\n",
    "**At this point, you can go back to the workshop guide to start Part 2 of the workshop.**\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Here are additional resources to help you dive deeper:\n",
    "\n",
    " - Setting up your own custom annotation job: https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/\n",
    "\n",
    " - Training a custom NER model using the Comprehend console: https://aws.amazon.com/blogs/machine-learning/extract-custom-entities-from-documents-in-their-native-format-with-amazon-comprehend/\n",
    "\n",
    " - API reference: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
